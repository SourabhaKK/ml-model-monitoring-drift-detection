This project implements a production-ready machine learning monitoring and drift detection system designed to detect data drift, prediction drift, and performance degradation in deployed ML models.

The system follows strict Test-Driven Development (TDD), clean architecture, and industry-standard monitoring patterns. It simulates real-world post-deployment ML challenges such as changing data distributions, silent model degradation, and observability gaps.

The project is designed as a resume-grade ML engineering portfolio project, emphasizing system design, testability, robustness, and extensibility rather than just model training.
